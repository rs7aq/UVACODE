{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gather a large corpus of data\n",
    "- Convert the text data into numerical representations by tokenizing words and building a vocablary of unique tokens\n",
    "- Choose a neural network architecture suitable for langauge modeling. Transformer based architectures like those used in GPT are commonly used. \n",
    "- Train the langauge model on the preprocessed text data\n",
    "- Evaluate the performance of the langauge model on a validation dataset\n",
    "- Deploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrof\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokens = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prompt = \"Tell me a story.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokens.encode(my_prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrof\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `100` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# higher temperature means more creative, lower temperature means more rigid, 1 is default value\n",
    "response = model.generate(tokenized_prompt, max_length = 250, num_return_sequences = 1, temperature = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decodes response into regular text\n",
    "generated_response = tokens.decode(response[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a story.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not sure what you're talking about.\n",
      "\n",
      "I'm not\n"
     ]
    }
   ],
   "source": [
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
